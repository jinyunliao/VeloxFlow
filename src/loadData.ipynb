{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49f73502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import io\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96508e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®åº“è¿žæŽ¥é…ç½®\n",
    "DB_USER = 'ecommerce_user'\n",
    "DB_PASSWORD = 'password'\n",
    "DB_HOST = '127.0.0.1'\n",
    "DB_PORT = '5432'\n",
    "DB_NAME = 'ecommerce_db'\n",
    "DATABASE_URI = f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "path_base = '../data/'\n",
    "CHUNK_SIZE = 10000     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1513abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fast_import_with_copy(DATABASE_URI,CSV_FILE_PATH,TABLE_NAME,CHUNK_SIZE):\n",
    "    # 1. Create the engine and obtain the original connection.\n",
    "    engine = create_engine(DATABASE_URI)\n",
    "    \n",
    "    # Obtain the underlying psycopg2 connection object (raw_connection).\n",
    "    # Because `copy_expert` is a psycopg2 method, not a standard SQLAlchemy method,\n",
    "    conn = engine.raw_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    print(f\"ðŸš€ Start high-speed import: {CSV_FILE_PATH} (records each time {CHUNK_SIZE} num)...\")\n",
    "    start_time = time.time()\n",
    "    total_rows = 0\n",
    "    chunk_count = 0\n",
    "\n",
    "    try:\n",
    "        # 2. Read CSV in chunks\n",
    "        with pd.read_csv(CSV_FILE_PATH, chunksize=CHUNK_SIZE) as reader:\n",
    "            for chunk in reader:\n",
    "                chunk_count += 1\n",
    "                \n",
    "                # --- Data processing area (optional) ---\n",
    "                # Here you can use Pandas to modify data.\n",
    "                # chunk['new_col'] = chunk['price'] * 2\n",
    "                # -------------------------\n",
    "\n",
    "                # 3. Convert a DataFrame to in-memory CSV format (StringIO)\n",
    "                output = io.StringIO()\n",
    "                chunk.to_csv(\n",
    "                    output, \n",
    "                    sep='\\t',       # Use tabs as separators to avoid escaping issues caused by commas.\n",
    "                    index=False,    # No index included\n",
    "                    header=False    # Table headers are not included (COPY does not require table headers).\n",
    "                )\n",
    "                output.seek(0) # The pointer returns to the head, ready to read.\n",
    "\n",
    "                # 4. Execute the COPY command\n",
    "                # STDIN indicates reading from standard input (which is our output memory stream).\n",
    "                try:\n",
    "                    cur.copy_expert(\n",
    "                        f\"COPY {TABLE_NAME} FROM STDIN WITH (FORMAT CSV, DELIMITER '\\t')\",\n",
    "                        output\n",
    "                    )\n",
    "                    conn.commit() # Commit transaction\n",
    "                except Exception as db_err:\n",
    "                    conn.rollback() # If an error occurs, rollback.\n",
    "                    print(f\"âŒ Database write error: {db_err}\")\n",
    "                    raise\n",
    "\n",
    "                rows_in_chunk = len(chunk)\n",
    "                total_rows += rows_in_chunk\n",
    "                print(f\"   -> The {chunk_count} batch of writes has completed ({rows_in_chunk} entries)\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"ðŸŽ‰ Import completed quickly! Total rows: {total_rows} time taken: {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: File not found{CSV_FILE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ An unknown error occurred: {e}\")\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "576d25b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olist_customers\n",
      "olist_customers_dataset.csv\n",
      "ðŸš€ Start high-speed import: /home/jason/data/Olist_baxi_ecomme_data/olist_customers_dataset.csv (records each time 10000 num)...\n",
      "   -> The 1 batch of writes has completed (10000 entries)\n",
      "   -> The 2 batch of writes has completed (10000 entries)\n",
      "   -> The 3 batch of writes has completed (10000 entries)\n",
      "   -> The 4 batch of writes has completed (10000 entries)\n",
      "   -> The 5 batch of writes has completed (10000 entries)\n",
      "   -> The 6 batch of writes has completed (10000 entries)\n",
      "   -> The 7 batch of writes has completed (10000 entries)\n",
      "   -> The 8 batch of writes has completed (10000 entries)\n",
      "   -> The 9 batch of writes has completed (10000 entries)\n",
      "   -> The 10 batch of writes has completed (9441 entries)\n",
      "ðŸŽ‰ Import completed quickly! Total rows: 99441 time taken: 1.45 seconds.\n",
      "olist_geolocation\n",
      "olist_geolocation_dataset.csv\n",
      "ðŸš€ Start high-speed import: /home/jason/data/Olist_baxi_ecomme_data/olist_geolocation_dataset.csv (records each time 10000 num)...\n",
      "   -> The 1 batch of writes has completed (10000 entries)\n",
      "   -> The 2 batch of writes has completed (10000 entries)\n",
      "   -> The 3 batch of writes has completed (10000 entries)\n",
      "   -> The 4 batch of writes has completed (10000 entries)\n",
      "   -> The 5 batch of writes has completed (10000 entries)\n",
      "   -> The 6 batch of writes has completed (10000 entries)\n",
      "   -> The 7 batch of writes has completed (10000 entries)\n",
      "   -> The 8 batch of writes has completed (10000 entries)\n",
      "   -> The 9 batch of writes has completed (10000 entries)\n",
      "   -> The 10 batch of writes has completed (10000 entries)\n",
      "   -> The 11 batch of writes has completed (10000 entries)\n",
      "   -> The 12 batch of writes has completed (10000 entries)\n",
      "   -> The 13 batch of writes has completed (10000 entries)\n",
      "   -> The 14 batch of writes has completed (10000 entries)\n",
      "   -> The 15 batch of writes has completed (10000 entries)\n",
      "   -> The 16 batch of writes has completed (10000 entries)\n",
      "   -> The 17 batch of writes has completed (10000 entries)\n",
      "   -> The 18 batch of writes has completed (10000 entries)\n",
      "   -> The 19 batch of writes has completed (10000 entries)\n",
      "   -> The 20 batch of writes has completed (10000 entries)\n",
      "   -> The 21 batch of writes has completed (10000 entries)\n",
      "   -> The 22 batch of writes has completed (10000 entries)\n",
      "   -> The 23 batch of writes has completed (10000 entries)\n",
      "   -> The 24 batch of writes has completed (10000 entries)\n",
      "   -> The 25 batch of writes has completed (10000 entries)\n",
      "   -> The 26 batch of writes has completed (10000 entries)\n",
      "   -> The 27 batch of writes has completed (10000 entries)\n",
      "   -> The 28 batch of writes has completed (10000 entries)\n",
      "   -> The 29 batch of writes has completed (10000 entries)\n",
      "   -> The 30 batch of writes has completed (10000 entries)\n",
      "   -> The 31 batch of writes has completed (10000 entries)\n",
      "   -> The 32 batch of writes has completed (10000 entries)\n",
      "   -> The 33 batch of writes has completed (10000 entries)\n",
      "   -> The 34 batch of writes has completed (10000 entries)\n",
      "   -> The 35 batch of writes has completed (10000 entries)\n",
      "   -> The 36 batch of writes has completed (10000 entries)\n",
      "   -> The 37 batch of writes has completed (10000 entries)\n",
      "   -> The 38 batch of writes has completed (10000 entries)\n",
      "   -> The 39 batch of writes has completed (10000 entries)\n",
      "   -> The 40 batch of writes has completed (10000 entries)\n",
      "   -> The 41 batch of writes has completed (10000 entries)\n",
      "   -> The 42 batch of writes has completed (10000 entries)\n",
      "   -> The 43 batch of writes has completed (10000 entries)\n",
      "   -> The 44 batch of writes has completed (10000 entries)\n",
      "   -> The 45 batch of writes has completed (10000 entries)\n",
      "   -> The 46 batch of writes has completed (10000 entries)\n",
      "   -> The 47 batch of writes has completed (10000 entries)\n",
      "   -> The 48 batch of writes has completed (10000 entries)\n",
      "   -> The 49 batch of writes has completed (10000 entries)\n",
      "   -> The 50 batch of writes has completed (10000 entries)\n",
      "   -> The 51 batch of writes has completed (10000 entries)\n",
      "   -> The 52 batch of writes has completed (10000 entries)\n",
      "   -> The 53 batch of writes has completed (10000 entries)\n",
      "   -> The 54 batch of writes has completed (10000 entries)\n",
      "   -> The 55 batch of writes has completed (10000 entries)\n",
      "   -> The 56 batch of writes has completed (10000 entries)\n",
      "   -> The 57 batch of writes has completed (10000 entries)\n",
      "   -> The 58 batch of writes has completed (10000 entries)\n",
      "   -> The 59 batch of writes has completed (10000 entries)\n",
      "   -> The 60 batch of writes has completed (10000 entries)\n",
      "   -> The 61 batch of writes has completed (10000 entries)\n",
      "   -> The 62 batch of writes has completed (10000 entries)\n",
      "   -> The 63 batch of writes has completed (10000 entries)\n",
      "   -> The 64 batch of writes has completed (10000 entries)\n",
      "   -> The 65 batch of writes has completed (10000 entries)\n",
      "   -> The 66 batch of writes has completed (10000 entries)\n",
      "   -> The 67 batch of writes has completed (10000 entries)\n",
      "   -> The 68 batch of writes has completed (10000 entries)\n",
      "   -> The 69 batch of writes has completed (10000 entries)\n",
      "   -> The 70 batch of writes has completed (10000 entries)\n",
      "   -> The 71 batch of writes has completed (10000 entries)\n",
      "   -> The 72 batch of writes has completed (10000 entries)\n",
      "   -> The 73 batch of writes has completed (10000 entries)\n",
      "   -> The 74 batch of writes has completed (10000 entries)\n",
      "   -> The 75 batch of writes has completed (10000 entries)\n",
      "   -> The 76 batch of writes has completed (10000 entries)\n",
      "   -> The 77 batch of writes has completed (10000 entries)\n",
      "   -> The 78 batch of writes has completed (10000 entries)\n",
      "   -> The 79 batch of writes has completed (10000 entries)\n",
      "   -> The 80 batch of writes has completed (10000 entries)\n",
      "   -> The 81 batch of writes has completed (10000 entries)\n",
      "   -> The 82 batch of writes has completed (10000 entries)\n",
      "   -> The 83 batch of writes has completed (10000 entries)\n",
      "   -> The 84 batch of writes has completed (10000 entries)\n",
      "   -> The 85 batch of writes has completed (10000 entries)\n",
      "   -> The 86 batch of writes has completed (10000 entries)\n",
      "   -> The 87 batch of writes has completed (10000 entries)\n",
      "   -> The 88 batch of writes has completed (10000 entries)\n",
      "   -> The 89 batch of writes has completed (10000 entries)\n",
      "   -> The 90 batch of writes has completed (10000 entries)\n",
      "   -> The 91 batch of writes has completed (10000 entries)\n",
      "   -> The 92 batch of writes has completed (10000 entries)\n",
      "   -> The 93 batch of writes has completed (10000 entries)\n",
      "   -> The 94 batch of writes has completed (10000 entries)\n",
      "   -> The 95 batch of writes has completed (10000 entries)\n",
      "   -> The 96 batch of writes has completed (10000 entries)\n",
      "   -> The 97 batch of writes has completed (10000 entries)\n",
      "   -> The 98 batch of writes has completed (10000 entries)\n",
      "   -> The 99 batch of writes has completed (10000 entries)\n",
      "   -> The 100 batch of writes has completed (10000 entries)\n",
      "   -> The 101 batch of writes has completed (163 entries)\n",
      "ðŸŽ‰ Import completed quickly! Total rows: 1000163 time taken: 8.07 seconds.\n",
      "olist_order_items\n",
      "olist_order_items_dataset.csv\n",
      "ðŸš€ Start high-speed import: /home/jason/data/Olist_baxi_ecomme_data/olist_order_items_dataset.csv (records each time 10000 num)...\n",
      "   -> The 1 batch of writes has completed (10000 entries)\n",
      "   -> The 2 batch of writes has completed (10000 entries)\n",
      "   -> The 3 batch of writes has completed (10000 entries)\n",
      "   -> The 4 batch of writes has completed (10000 entries)\n",
      "   -> The 5 batch of writes has completed (10000 entries)\n",
      "   -> The 6 batch of writes has completed (10000 entries)\n",
      "   -> The 7 batch of writes has completed (10000 entries)\n",
      "   -> The 8 batch of writes has completed (10000 entries)\n",
      "   -> The 9 batch of writes has completed (10000 entries)\n",
      "   -> The 10 batch of writes has completed (10000 entries)\n",
      "   -> The 11 batch of writes has completed (10000 entries)\n",
      "   -> The 12 batch of writes has completed (2650 entries)\n",
      "ðŸŽ‰ Import completed quickly! Total rows: 112650 time taken: 1.48 seconds.\n",
      "olist_order\n",
      "olist_orders_dataset.csv\n",
      "ðŸš€ Start high-speed import: /home/jason/data/Olist_baxi_ecomme_data/olist_orders_dataset.csv (records each time 10000 num)...\n",
      "   -> The 1 batch of writes has completed (10000 entries)\n",
      "   -> The 2 batch of writes has completed (10000 entries)\n",
      "   -> The 3 batch of writes has completed (10000 entries)\n",
      "   -> The 4 batch of writes has completed (10000 entries)\n",
      "   -> The 5 batch of writes has completed (10000 entries)\n",
      "   -> The 6 batch of writes has completed (10000 entries)\n",
      "   -> The 7 batch of writes has completed (10000 entries)\n",
      "   -> The 8 batch of writes has completed (10000 entries)\n",
      "   -> The 9 batch of writes has completed (10000 entries)\n",
      "   -> The 10 batch of writes has completed (9441 entries)\n",
      "ðŸŽ‰ Import completed quickly! Total rows: 99441 time taken: 2.01 seconds.\n",
      "olist_order_payments\n",
      "olist_order_payments_dataset.csv\n",
      "ðŸš€ Start high-speed import: /home/jason/data/Olist_baxi_ecomme_data/olist_order_payments_dataset.csv (records each time 10000 num)...\n",
      "   -> The 1 batch of writes has completed (10000 entries)\n",
      "   -> The 2 batch of writes has completed (10000 entries)\n",
      "   -> The 3 batch of writes has completed (10000 entries)\n",
      "   -> The 4 batch of writes has completed (10000 entries)\n",
      "   -> The 5 batch of writes has completed (10000 entries)\n",
      "   -> The 6 batch of writes has completed (10000 entries)\n",
      "   -> The 7 batch of writes has completed (10000 entries)\n",
      "   -> The 8 batch of writes has completed (10000 entries)\n",
      "   -> The 9 batch of writes has completed (10000 entries)\n",
      "   -> The 10 batch of writes has completed (10000 entries)\n",
      "   -> The 11 batch of writes has completed (3886 entries)\n",
      "ðŸŽ‰ Import completed quickly! Total rows: 103886 time taken: 0.67 seconds.\n",
      "olist_order_reviews\n",
      "olist_order_reviews_dataset.csv\n",
      "ðŸš€ Start high-speed import: /home/jason/data/Olist_baxi_ecomme_data/olist_order_reviews_dataset.csv (records each time 10000 num)...\n",
      "   -> The 1 batch of writes has completed (10000 entries)\n",
      "   -> The 2 batch of writes has completed (10000 entries)\n",
      "   -> The 3 batch of writes has completed (10000 entries)\n",
      "   -> The 4 batch of writes has completed (10000 entries)\n",
      "   -> The 5 batch of writes has completed (10000 entries)\n",
      "   -> The 6 batch of writes has completed (10000 entries)\n",
      "   -> The 7 batch of writes has completed (10000 entries)\n",
      "   -> The 8 batch of writes has completed (10000 entries)\n",
      "   -> The 9 batch of writes has completed (10000 entries)\n",
      "   -> The 10 batch of writes has completed (9224 entries)\n",
      "ðŸŽ‰ Import completed quickly! Total rows: 99224 time taken: 1.37 seconds.\n",
      "olist_products\n",
      "olist_products_dataset.csv\n",
      "ðŸš€ Start high-speed import: /home/jason/data/Olist_baxi_ecomme_data/olist_products_dataset.csv (records each time 10000 num)...\n",
      "   -> The 1 batch of writes has completed (10000 entries)\n",
      "   -> The 2 batch of writes has completed (10000 entries)\n",
      "   -> The 3 batch of writes has completed (10000 entries)\n",
      "   -> The 4 batch of writes has completed (2951 entries)\n",
      "ðŸŽ‰ Import completed quickly! Total rows: 32951 time taken: 0.51 seconds.\n",
      "olist_sellers\n",
      "olist_sellers_dataset.csv\n",
      "ðŸš€ Start high-speed import: /home/jason/data/Olist_baxi_ecomme_data/olist_sellers_dataset.csv (records each time 10000 num)...\n",
      "   -> The 1 batch of writes has completed (3095 entries)\n",
      "ðŸŽ‰ Import completed quickly! Total rows: 3095 time taken: 0.03 seconds.\n",
      "product_category_name_translation\n",
      "product_category_name_translation.csv\n",
      "ðŸš€ Start high-speed import: /home/jason/data/Olist_baxi_ecomme_data/product_category_name_translation.csv (records each time 10000 num)...\n",
      "   -> The 1 batch of writes has completed (71 entries)\n",
      "ðŸŽ‰ Import completed quickly! Total rows: 71 time taken: 0.01 seconds.\n"
     ]
    }
   ],
   "source": [
    "list = [ {'table':'olist_customers','csv':'olist_customers_dataset.csv'},{'table':'olist_geolocation','csv':'olist_geolocation_dataset.csv'},\n",
    "{'table':'olist_order_items','csv':'olist_order_items_dataset.csv'} ,{'table':'olist_order','csv':'olist_orders_dataset.csv'},\n",
    "{'table':'olist_order_payments','csv':'olist_order_payments_dataset.csv'},{'table':'olist_order_reviews','csv':'olist_order_reviews_dataset.csv'},\n",
    "{'table':'olist_products','csv':'olist_products_dataset.csv'},{'table':'olist_sellers','csv':'olist_sellers_dataset.csv'},{'table':'product_category_name_translation','csv':'product_category_name_translation.csv'}]\n",
    "for item in list:\n",
    "    print(item['table'])\n",
    "    print(item['csv'])\n",
    "    CSV_FILE_PATH =path_base+ item['csv']      # csv path \n",
    "    TABLE_NAME = item['table']\n",
    "    fast_import_with_copy(DATABASE_URI,CSV_FILE_PATH,TABLE_NAME,CHUNK_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
